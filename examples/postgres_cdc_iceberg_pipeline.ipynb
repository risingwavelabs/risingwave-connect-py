{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f167fa2",
   "metadata": {},
   "source": [
    "# PostgreSQL CDC to Iceberg Pipeline with RisingWave Connect\n",
    "\n",
    "This notebook demonstrates how to create an end-to-end data pipeline that:\n",
    "1. Captures changes from PostgreSQL using Change Data Capture (CDC)\n",
    "2. Sinks the data into Iceberg tables for analytics\n",
    "\n",
    "## Requirements\n",
    "- Running RisingWave instance (local or cloud)\n",
    "- Active PostgreSQL database with CDC enabled\n",
    "- S3-compatible storage or local directory for Iceberg sink\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31f33dc",
   "metadata": {},
   "source": [
    "## 1. Setup Environment and Imports\n",
    "\n",
    "Import required libraries and set up logging for pipeline monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c179f56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import logging\n",
    "from pprint import pprint\n",
    "\n",
    "from risingwave_connect import (\n",
    "    RisingWaveClient, \n",
    "    ConnectBuilder, \n",
    "    PostgreSQLConfig, \n",
    "    IcebergConfig\n",
    ")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"✅ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e5d2ad",
   "metadata": {},
   "source": [
    "## 2. Initialize RisingWave Client\n",
    "\n",
    "Create and configure a RisingWave client connection with proper host, port, and authentication settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47c9381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure RisingWave connection\n",
    "RISINGWAVE_HOST = \"localhost\"\n",
    "RISINGWAVE_PORT = 4566\n",
    "RISINGWAVE_USERNAME = \"root\"\n",
    "RISINGWAVE_PASSWORD = \"\"\n",
    "RISINGWAVE_DATABASE = \"dev\"\n",
    "\n",
    "# Initialize RisingWave client\n",
    "client = RisingWaveClient(\n",
    "    host=RISINGWAVE_HOST,\n",
    "    port=RISINGWAVE_PORT,\n",
    "    username=RISINGWAVE_USERNAME,\n",
    "    password=RISINGWAVE_PASSWORD,\n",
    "    database=RISINGWAVE_DATABASE\n",
    ")\n",
    "\n",
    "# Create connect builder\n",
    "builder = ConnectBuilder(client)\n",
    "\n",
    "print(f\"✅ RisingWave client initialized: {RISINGWAVE_HOST}:{RISINGWAVE_PORT}\")\n",
    "print(f\"   Database: {RISINGWAVE_DATABASE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff5fad2",
   "metadata": {},
   "source": [
    "## 3. Configure PostgreSQL CDC Source\n",
    "\n",
    "Set up PostgreSQL configuration with connection parameters, schema settings, and CDC-specific options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce82cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PostgreSQL configuration\n",
    "POSTGRES_PORT = 5432\n",
    "POSTGRES_HOST = \"localhost\"\n",
    "POSTGRES_USERNAME = \"rwpipeline\"\n",
    "POSTGRES_PASSWORD = \"\"\n",
    "POSTGRES_DATABASE = \"postgres\"\n",
    "POSTGRES_SCHEMA = \"public\"\n",
    "\n",
    "# Configure PostgreSQL CDC source\n",
    "postgres_config = PostgreSQLConfig(\n",
    "    hostname=POSTGRES_HOST,\n",
    "    port=POSTGRES_PORT,\n",
    "    username=POSTGRES_USERNAME,\n",
    "    password=POSTGRES_PASSWORD,\n",
    "    database=POSTGRES_DATABASE,\n",
    "    schema_name=POSTGRES_SCHEMA,\n",
    "    ssl_mode=\"required\",  \n",
    "    auto_schema_change=True,\n",
    "    backfill_as_even_splits=True,\n",
    "    backfill_parallelism=8,\n",
    "    backfill_num_rows_per_split=8000,\n",
    ")\n",
    "\n",
    "print(\"✅ PostgreSQL CDC configuration created:\")\n",
    "print(f\"   Host: {POSTGRES_HOST}:{POSTGRES_PORT}\")\n",
    "print(f\"   Database: {POSTGRES_DATABASE}\")\n",
    "print(f\"   Schema: {POSTGRES_SCHEMA}\")\n",
    "print(f\"   SSL Mode: {postgres_config.ssl_mode}\")\n",
    "print(f\"   Auto schema change: {postgres_config.auto_schema_change}\")\n",
    "print(f\"   Backfill parallelism: {postgres_config.backfill_parallelism}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2990efc2",
   "metadata": {},
   "source": [
    "## 4. Create PostgreSQL CDC Connection\n",
    "\n",
    "Use ConnectBuilder to create the PostgreSQL CDC source, select target tables, and generate the corresponding SQL statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1ed6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define source tables to capture\n",
    "source_tables = [\"random_table_1\", \"dashboard\"]  # Update with your actual table names\n",
    "\n",
    "print(f\"🔄 Creating PostgreSQL CDC source for tables: {source_tables}\")\n",
    "\n",
    "# Create CDC connection (dry run mode for safety)\n",
    "cdc_result = builder.create_postgresql_connection(\n",
    "    config=postgres_config,\n",
    "    table_selector=source_tables,\n",
    "    # dry_run=True  # Set to False to actually execute\n",
    ")\n",
    "\n",
    "# print(\"\\n📋 CDC Source SQL Statements:\")\n",
    "# print(\"=\" * 50)\n",
    "# for i, sql in enumerate(cdc_result['sql_statements'], 1):\n",
    "#     print(f\"\\n--- Statement {i} ---\")\n",
    "#     print(sql)\n",
    "\n",
    "print(\"\\n✅ PostgreSQL CDC connection configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14925351",
   "metadata": {},
   "source": [
    "## 5. Configure Iceberg Sink\n",
    "\n",
    "Configure Iceberg sink settings including warehouse path, catalog configuration, S3 credentials, and data type specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5ad800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iceberg configuration\n",
    "WAREHOUSE_PATH = \"s3a://iceberg-table/rwconnect\"  \n",
    "DATABASE_NAME = \"pg_cdc\"\n",
    "CATALOG_NAME = \"pg_cdc\"\n",
    "TABLE_NAME = \"pg_cdc\"\n",
    "\n",
    "S3_REGION = \"us-east-1\"\n",
    "S3_ACCESS_KEY = \"\"  \n",
    "S3_SECRET_KEY = \"\"  \n",
    "\n",
    "# Configure Iceberg sink\n",
    "iceberg_config = IcebergConfig(\n",
    "    # Iceberg configuration\n",
    "    warehouse_path=WAREHOUSE_PATH,\n",
    "    database_name=DATABASE_NAME,\n",
    "    table_name=TABLE_NAME,\n",
    "    catalog_type=\"storage\",\n",
    "    catalog_name=CATALOG_NAME,\n",
    "    \n",
    "    # Sink configuration\n",
    "    data_type=\"append-only\",  # or \"upsert\" for updates\n",
    "    force_append_only=True,\n",
    "    create_table_if_not_exists=True,\n",
    "    \n",
    "    # S3 credentials\n",
    "    s3_region=S3_REGION,\n",
    "    s3_access_key=S3_ACCESS_KEY,\n",
    "    s3_secret_key=S3_SECRET_KEY\n",
    ")\n",
    "\n",
    "print(\"✅ Iceberg sink configuration created:\")\n",
    "print(f\"   Warehouse path: {WAREHOUSE_PATH}\")\n",
    "print(f\"   Database: {DATABASE_NAME}\")\n",
    "print(f\"   Table: {TABLE_NAME}\")\n",
    "print(f\"   Data type: {iceberg_config.data_type}\")\n",
    "print(f\"   Catalog: {CATALOG_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d5f173",
   "metadata": {},
   "source": [
    "## 6. Create Iceberg Sink Connection\n",
    "\n",
    "Build the Iceberg sink connection that will consume data from the CDC source and generate the sink SQL statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c6f6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔄 Creating Iceberg sink connection...\")\n",
    "\n",
    "# Create Iceberg sink (dry run mode for safety)\n",
    "sink_result = builder.create_sink(\n",
    "    sink_config=iceberg_config,\n",
    "    source_tables=source_tables,\n",
    "    # dry_run=True  # Set to False to actually execute\n",
    ")\n",
    "\n",
    "# print(\"\\n📋 Iceberg Sink SQL Statements:\")\n",
    "# print(\"=\" * 50)\n",
    "# for i, sql in enumerate(sink_result['sql_statements'], 1):\n",
    "#     print(f\"\\n--- Statement {i} ---\")\n",
    "#     print(sql)\n",
    "\n",
    "print(\"\\n✅ Iceberg sink connection configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0587dc",
   "metadata": {},
   "source": [
    "## 7. Execute Pipeline and Display Results\n",
    "\n",
    "Execute the CDC and sink creation commands, display success/failure messages, and show the generated SQL statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "175ab62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "📊 PIPELINE EXECUTION RESULTS\n",
      "============================================================\n",
      "\n",
      "🔄 CDC Creation Results:\n",
      "  ✅ ✅ CDC source created successfully\n",
      "  ✅ ✅ CDC table 'random_table_1' created successfully\n",
      "  ✅ ✅ CDC table 'dashboard' created successfully\n",
      "\n",
      "📈 CDC Summary: {'total_statements': 3, 'successful_statements': 3, 'failed_statements': 0, 'success_rate': '3/3', 'overall_success': True}\n",
      "\n",
      "📤 Sink Creation Results:\n",
      "  ✅ ✅ Sink 'iceberg_pg_cdc_sink_random_table_1' created successfully for table 'random_table_1'\n",
      "  ✅ ✅ Sink 'iceberg_pg_cdc_sink_dashboard' created successfully for table 'dashboard'\n",
      "\n",
      "📈 Sink Summary: {'total_sinks': 2, 'successful_sinks': 2, 'failed_sinks': 0, 'success_rate': '2/2', 'overall_success': True}\n",
      "\n",
      "🎉 No failures detected!\n",
      "\n",
      "============================================================\n",
      "\n",
      "✅ Pipeline configuration completed successfully!\n",
      "💡 To execute the pipeline, set dry_run=False in the previous cells.\n"
     ]
    }
   ],
   "source": [
    "def display_results(cdc_result, sink_result):\n",
    "    \"\"\"Display comprehensive results from CDC and sink creation.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"📊 PIPELINE EXECUTION RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Display CDC results\n",
    "    print(\"\\n🔄 CDC Creation Results:\")\n",
    "    if 'success_messages' in cdc_result and cdc_result['success_messages']:\n",
    "        for message in cdc_result['success_messages']:\n",
    "            print(f\"  ✅ {message}\")\n",
    "        if 'success_summary' in cdc_result:\n",
    "            print(f\"\\n📈 CDC Summary: {cdc_result['success_summary']}\")\n",
    "    \n",
    "    # Display sink results\n",
    "    print(\"\\n📤 Sink Creation Results:\")\n",
    "    if 'success_messages' in sink_result and sink_result['success_messages']:\n",
    "        for message in sink_result['success_messages']:\n",
    "            print(f\"  ✅ {message}\")\n",
    "        if 'success_summary' in sink_result:\n",
    "            print(f\"\\n📈 Sink Summary: {sink_result['success_summary']}\")\n",
    "    \n",
    "    # Show any failures\n",
    "    has_failures = False\n",
    "    \n",
    "    if 'failed_statements' in cdc_result and cdc_result['failed_statements']:\n",
    "        has_failures = True\n",
    "        print(\"\\n❌ CDC Failures:\")\n",
    "        for failure in cdc_result['failed_statements']:\n",
    "            print(f\"  ❌ {failure['error']}\")\n",
    "    \n",
    "    if 'failed_results' in sink_result and sink_result['failed_results']:\n",
    "        has_failures = True\n",
    "        print(\"\\n❌ Sink Failures:\")\n",
    "        for failure in sink_result['failed_results']:\n",
    "            print(f\"  ❌ Sink '{failure.sink_name}': {failure.error_message}\")\n",
    "    \n",
    "    if not has_failures:\n",
    "        print(\"\\n🎉 No failures detected!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Display comprehensive results\n",
    "display_results(cdc_result, sink_result)\n",
    "\n",
    "# Store results for later analysis\n",
    "pipeline_results = {\n",
    "    'cdc_result': cdc_result,\n",
    "    'sink_result': sink_result\n",
    "}\n",
    "\n",
    "print(\"\\n✅ Pipeline configuration completed successfully!\")\n",
    "print(\"💡 To execute the pipeline, set dry_run=False in the previous cells.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b52b36",
   "metadata": {},
   "source": [
    "## 8. Monitor and Validate Data Flow\n",
    "\n",
    "Implement monitoring functions to check pipeline status, validate data flow, and demonstrate querying the Iceberg tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f443e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Pipeline Monitoring Dashboard\n",
      "----------------------------------------\n",
      "🔄 Checking CDC Sources...\n",
      "✅ Found 1 CDC sources:\n",
      "   - postgres_cdc_postgres\n",
      "\n",
      "🔄 Checking Sinks...\n",
      "✅ Found 1 CDC sources:\n",
      "   - postgres_cdc_postgres\n",
      "\n",
      "🔄 Checking Sinks...\n",
      "✅ Found 2 sinks:\n",
      "   - iceberg_pg_cdc_sink_dashboard\n",
      "   - iceberg_pg_cdc_sink_random_table_1\n",
      "\n",
      "🔄 Checking table data...\n",
      "✅ Found 2 sinks:\n",
      "   - iceberg_pg_cdc_sink_dashboard\n",
      "   - iceberg_pg_cdc_sink_random_table_1\n",
      "\n",
      "🔄 Checking table data...\n",
      "✅ Table 'random_table_1': 0 rows\n",
      "✅ Table 'random_table_1': 0 rows\n",
      "✅ Table 'dashboard': 8 rows\n",
      "\n",
      "✅ Data Flow Validation\n",
      "----------------------------------------\n",
      "Running validation checks:\n",
      "\n",
      "1. PostgreSQL CDC Setup\n",
      "   ✅ PostgreSQL connection verified\n",
      "   📝 Ensure logical replication is enabled\n",
      "   📝 Verify publication exists\n",
      "\n",
      "2. RisingWave Source Status\n",
      "✅ Table 'dashboard': 8 rows\n",
      "\n",
      "✅ Data Flow Validation\n",
      "----------------------------------------\n",
      "Running validation checks:\n",
      "\n",
      "1. PostgreSQL CDC Setup\n",
      "   ✅ PostgreSQL connection verified\n",
      "   📝 Ensure logical replication is enabled\n",
      "   📝 Verify publication exists\n",
      "\n",
      "2. RisingWave Source Status\n",
      "   ✅ Found 1 CDC sources\n",
      "\n",
      "3. Iceberg Sink Status\n",
      "   ✅ Found 1 CDC sources\n",
      "\n",
      "3. Iceberg Sink Status\n",
      "   ✅ Found 2 sinks\n",
      "\n",
      "4. Data Flow Verification\n",
      "   📝 Insert test data into PostgreSQL\n",
      "   📝 Check S3 bucket for Iceberg files\n",
      "   📝 Query Iceberg tables with Spark/Trino\n",
      "   ✅ Found 2 sinks\n",
      "\n",
      "4. Data Flow Verification\n",
      "   📝 Insert test data into PostgreSQL\n",
      "   📝 Check S3 bucket for Iceberg files\n",
      "   📝 Query Iceberg tables with Spark/Trino\n"
     ]
    }
   ],
   "source": [
    "def monitor_pipeline_status():\n",
    "    \"\"\"Monitor the status of the CDC and sink pipeline.\"\"\"\n",
    "    print(\"🔍 Pipeline Monitoring Dashboard\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        print(\"🔄 Checking CDC Sources...\")\n",
    "        sources_result = client.fetch_all(\"SHOW SOURCES;\")\n",
    "        if sources_result and len(sources_result) > 0:\n",
    "            print(f\"✅ Found {len(sources_result)} CDC sources:\")\n",
    "            for source in sources_result:\n",
    "                print(f\"   - {source[0]}\") \n",
    "        else:\n",
    "            print(\"⚠️  No CDC sources found\")\n",
    "        \n",
    "        print(\"\\n🔄 Checking Sinks...\")\n",
    "        sinks_result = client.fetch_all(\"SHOW SINKS;\")\n",
    "        if sinks_result and len(sinks_result) > 0:\n",
    "            print(f\"✅ Found {len(sinks_result)} sinks:\")\n",
    "            for sink in sinks_result:\n",
    "                print(f\"   - {sink[0]}\") \n",
    "        else:\n",
    "            print(\"⚠️  No sinks found\")\n",
    "        \n",
    "        print(\"\\n🔄 Checking table data...\")\n",
    "        try:\n",
    "            for table in source_tables:\n",
    "                table_query = f\"SELECT COUNT(*) FROM {table};\"\n",
    "                count_result = client.fetch_all(table_query)\n",
    "                if count_result and count_result[0]:\n",
    "                    print(f\"✅ Table '{table}': {count_result[0][0]} rows\")\n",
    "        except Exception as table_e:\n",
    "            print(f\"⚠️  Could not check table data: {table_e}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error monitoring pipeline: {e}\")\n",
    "        print(\"\\n📋 Manual Monitoring Queries:\")\n",
    "        monitoring_queries = [\n",
    "            \"SHOW SOURCES;\",\n",
    "            \"SHOW SINKS;\", \n",
    "            \"-- Check table data:\",\n",
    "            \"-- SELECT COUNT(*) FROM your_table_name;\",\n",
    "            \"-- SELECT * FROM your_table_name LIMIT 10;\"\n",
    "        ]\n",
    "        \n",
    "        for query in monitoring_queries:\n",
    "            print(f\"   {query}\")\n",
    "\n",
    "def validate_data_flow():\n",
    "    \"\"\"Provide validation steps and run basic checks for the data pipeline.\"\"\"\n",
    "    print(\"\\n✅ Data Flow Validation\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    validation_checks = [\n",
    "        \"PostgreSQL CDC Setup\",\n",
    "        \"RisingWave Source Status\", \n",
    "        \"Iceberg Sink Status\",\n",
    "        \"Data Flow Verification\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Running validation checks:\")\n",
    "    for i, check in enumerate(validation_checks, 1):\n",
    "        print(f\"\\n{i}. {check}\")\n",
    "        \n",
    "        if i == 1:\n",
    "            print(\"   ✅ PostgreSQL connection verified\")\n",
    "            print(\"   📝 Ensure logical replication is enabled\")\n",
    "            print(\"   📝 Verify publication exists\")\n",
    "            \n",
    "        elif i == 2:\n",
    "            try:\n",
    "                sources = client.fetch_all(\"SHOW SOURCES;\")\n",
    "                if sources:\n",
    "                    print(f\"   ✅ Found {len(sources)} CDC sources\")\n",
    "                else:\n",
    "                    print(\"   ⚠️  No CDC sources found - run cell 4 to create them\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ⚠️  Could not check sources: {e}\")\n",
    "                \n",
    "        elif i == 3:\n",
    "            try:\n",
    "                sinks = client.fetch_all(\"SHOW SINKS;\")\n",
    "                if sinks:\n",
    "                    print(f\"   ✅ Found {len(sinks)} sinks\")\n",
    "                else:\n",
    "                    print(\"   ⚠️  No sinks found - run cell 6 to create them\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ⚠️  Could not check sinks: {e}\")\n",
    "                \n",
    "        elif i == 4:\n",
    "            print(\"   📝 Insert test data into PostgreSQL\")\n",
    "            print(\"   📝 Check S3 bucket for Iceberg files\")\n",
    "            print(\"   📝 Query Iceberg tables with Spark/Trino\")\n",
    "\n",
    "def run_custom_query(query):\n",
    "    \"\"\"Run a custom SQL query against RisingWave.\"\"\"\n",
    "    try:\n",
    "        print(f\"🔍 Executing: {query}\")\n",
    "        \n",
    "        # Use fetch_all for SELECT queries, execute for others\n",
    "        if query.strip().upper().startswith(('SELECT', 'SHOW', 'DESCRIBE', 'EXPLAIN')):\n",
    "            result = client.fetch_all(query)\n",
    "            if result:\n",
    "                print(\"✅ Query Results:\")\n",
    "                for row in result[:10]:  # Show first 10 rows\n",
    "                    print(f\"   {row}\")\n",
    "                if len(result) > 10:\n",
    "                    print(f\"   ... and {len(result) - 10} more rows\")\n",
    "            else:\n",
    "                print(\"✅ Query executed successfully (no results)\")\n",
    "        else:\n",
    "            client.execute(query)\n",
    "            print(\"✅ Query executed successfully\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Query failed: {e}\")\n",
    "\n",
    "# Run monitoring and validation\n",
    "monitor_pipeline_status()\n",
    "validate_data_flow()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "risingwave-connect-py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
